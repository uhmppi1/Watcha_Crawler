{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import bs4\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"./chromedriver\"\n",
    "driver = webdriver.Chrome(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Untitled', encoding = 'utf-8') as json_file:\n",
    "    list_before = json.load(json_file)\n",
    "\n",
    "list_movie = list_before['movieListResult']['movieList']\n",
    "len_movie = len(list_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성우는 괜찮아에 대한 결과\n",
      "정답은 성우는 괜찮아, 2017, 한국\n",
      "검색 결과가 없음\n",
      "##########################################\n",
      "\n",
      "뷰티풀 보이에 대한 결과\n",
      "정답은 뷰티풀 보이, 2018, 미국\n",
      "뷰티풀 보이매칭 성공\n",
      "##########################################\n",
      "\n",
      "내 삶의 빛, 나의 모든 것\n",
      "나의 아름다운 소년아, \n",
      "눈부신 그때로 돌아와줘\n",
      " \n",
      "“아름다운 소년이 있었어. 불행히도 소년은 끔찍한 병에 걸렸어”\n",
      "열렬한 독서가이자 재능 있는 예술가, 운동을 좋아하는 모범생 닉. 12살 때 손을 댄 약물에 어느새 중독자가 되었다.\n",
      "\n",
      "“다행히도 아직 이야기는 끝나지 않았어”\n",
      "아들의 중독이 자기 때문이 아닐까 자책하던 데이비드. 눈물 흘리며 포기하는 대신, 세상의 전부이자 모든 것인 닉의 손을 끝까지 붙잡는데…\n",
      "\n",
      "\n",
      "더 몬스터에 대한 결과\n",
      "정답은 더 몬스터, 2016, 미국\n",
      "더 몬스터매칭 성공\n",
      "##########################################\n",
      "\n",
      "스토리 없음\n",
      "\n",
      "\n",
      "침입자들에 대한 결과\n",
      "정답은 침입자들, 2019, 홍콩\n",
      "침입자들매칭 성공\n",
      "##########################################\n",
      "\n",
      "기록적인 폭우가 쏟아지는 크리스마스 전야. 사인을 밝히는 시체 부검소에는 한 젊은 여인의 총살된 시신이 들어온다. 그리고 복면을 쓴 채 들이닥친 3명의 침입자! 법의학자 진가호와 인턴 교림을 협박하는데, 과연 그들이 부검소를 침입한 충격적인 비밀은...?\n",
      "\n",
      "\n",
      "극장판 타오르지마버스터-블랙어썰트의 귀환에 대한 결과\n",
      "정답은 극장판 타오르지마버스터-블랙어썰트의 귀환, 2019, 한국\n",
      "검색 결과가 없음\n",
      "##########################################\n",
      "\n",
      "아마추어 유부녀에 대한 결과\n",
      "정답은 아마추어 유부녀, 2019, 일본\n",
      "검색 결과가 없음\n",
      "##########################################\n",
      "\n",
      "거근에 중독된 미모의 내 아내에 대한 결과\n",
      "정답은 거근에 중독된 미모의 내 아내, 2019, 일본\n",
      "검색 결과가 없음\n",
      "##########################################\n",
      "\n",
      "애월에 대한 결과\n",
      "정답은 애월, 2019, 한국\n",
      "애월매칭 성공\n",
      "##########################################\n",
      "\n",
      "오토바이로 전국 일주를 하던 수현은 제주도 애월에서 비운의 사고로 목숨을 잃는다. 사고 이후, 수현의 연인 소월은 그리움에 잠겨 애월을 떠나지 않은 채 살아가고 있다. 한편, 둘의 가장 친한 친구였던 철이는 수현이 죽기 전 보낸 편지를 3년이 지나서 받게 되고 무작정 애월로 떠난다. 소월을 찾아와 그녀의 집에 잠시 머물게 되는 철이는 마을 사람들과 함께 어울리며 애월에서의 소소한 일상을 보내고, 철이와 소월은 그렇게 함께 지내며 죽은 수현에 대한 그리움을 각자의 방식으로 극복하기 시작하는데...\n",
      "\n",
      "\n",
      "갈까부다에 대한 결과\n",
      "정답은 갈까부다, 2018, 한국\n",
      "갈까부다매칭 성공\n",
      "##########################################\n",
      "\n",
      "43살 영화감독 고봉수는 자신보다 18살 어린 여자친구 최은비를 위해 국악 다큐멘터리를 기획한다. 지금까지 여자친구에게 제대로 된 선물을 한 적이 없던 고봉수는 여자친구가 크게 감동 할 것이라 생각해 야심차게 준비하지만 다큐멘터리 제작을 위해 여자친구의 친구들과 가족들을 인터뷰를 하는 과정에서 온갖 욕과 모욕적인 말을 듣게 되고 극심한 반대에 부딪힌다. 좌절한 고봉수는 최은비의 아버지를 자신의 편으로 만들면 모든 것이 해결될 것이라 생각해 최은비의 아버지를 찾아가지만 되려 최은비의 아버지는 고봉수를 구타하며 크게 분노한다. 자신과 여자친구의 가족들 뿐 아니라 주변 모든 지인들이 반대를 하는 연애. 결국 둘은 반대에 지쳐 헤어지게 된다. 고봉수는 마지막 선물로써 여자친구의 국악 다큐멘터리를 완성시킨다.\n",
      "\n",
      "\n",
      "섬에 대한 결과\n",
      "정답은 섬, 2016, 한국\n",
      "검색 결과는 있지만 매칭되는 게 없음\n",
      "##########################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len_movie):\n",
    "    count=0\n",
    "    name_movie_pre = list_movie[i]['movieNm']\n",
    "    name_movie = re.sub(' ','%20',name_movie_pre)\n",
    "    year_movie = list_movie[i]['prdtYear']\n",
    "    nation_movie = list_movie[i]['repNationNm']\n",
    "    driver.get('https://watcha.com/ko-KR/search?query='+name_movie)\n",
    "    now_html = driver.page_source\n",
    "    now_source = BeautifulSoup(now_html, 'lxml')\n",
    "    title_prev = now_source.find_all('li', {'class':\"css-106b4k6-Self e3fgkal0\" })\n",
    "    len_same = len(title_prev)\n",
    "    \n",
    "    print(name_movie_pre + '에 대한 결과')\n",
    "    print('정답은 {}, {}, {}'.format(name_movie_pre, year_movie, nation_movie))\n",
    "    \n",
    "    for j in range(len_same):\n",
    "        query_title = title_prev[j].a.find_all('div')[2].find_all('div')[0].text\n",
    "        query_movie = title_prev[j].a.find_all('div')[2].find_all('div')[1].text.split()[0]\n",
    "        query_nation = title_prev[j].a.find_all('div')[2].find_all('div')[1].text.split()[2]\n",
    "        \n",
    "        if(query_title == name_movie_pre and query_movie == year_movie and query_nation == nation_movie):\n",
    "            print(name_movie_pre + '매칭 성공\\n##########################################\\n')\n",
    "            driver.get('https://watcha.com' + title_prev[j].a['href'])\n",
    "            now_html = driver.page_source\n",
    "            now_source = BeautifulSoup(now_html, 'lxml')\n",
    "            overview_prev = now_source.find_all('div', {'class':\"css-1jyvmaq-ViewMore et86el20\"})[0]\n",
    "            overview_prev2 = overview_prev.a['href']\n",
    "            time.sleep(0.05)\n",
    "            driver.get('https://watcha.com' + overview_prev2)\n",
    "            now_html = driver.page_source\n",
    "            now_source = BeautifulSoup(now_html, 'lxml')\n",
    "            overview = now_source.find_all('dd', {'class':\"css-77qx4t-SummaryDetail e1kvv3954\"})[0].text\n",
    "            print(overview+'\\n\\n')\n",
    "            count+=1\n",
    "            break\n",
    "        \n",
    "    if(count<1):\n",
    "        if(len_same<1):\n",
    "            print('검색 결과가 없음\\n##########################################\\n')\n",
    "        else:\n",
    "            print('검색 결과는 있지만 매칭되는 게 없음\\n##########################################\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class watcha_crawler(webdriver.Chrome):\n",
    "    \n",
    "    def find_matching(list_movie, i): # 영화 리스트와 순서를 인풋으로 받으면\n",
    "        \n",
    "        name_movie_pre = list_movie[i]['movieNm'] # 영화 리스트에서 매칭 쿼리를 잡고\n",
    "        name_movie = re.sub(' ','%20',name_movie_pre)\n",
    "        year_movie = list_movie[i]['prdtYear'] \n",
    "        nation_movie = list_movie[i]['repNationNm'] \n",
    "        watcha_crawler.get('https://watcha.com/ko-KR/search?query='+name_movie) # 왓챠에서 영화를 검색해 본다.\n",
    "        now_html = watcha_crawler.page_source\n",
    "        now_source = BeautifulSoup(now_html, 'lxml')\n",
    "        title_prev = now_source.find_all('li', {'class':\"css-106b4k6-Self e3fgkal0\" }) # 검색 결과 다양한 영화가 나오니 listup을 한다.\n",
    "        len_same = len(title_prev)\n",
    "        \n",
    "        print(name_movie_pre + '에 대한 결과')\n",
    "        print('정답은 {}, {}, {}'.format(name_movie_pre, year_movie, nation_movie))\n",
    "\n",
    "        for j in range(len_same): #제목, 제작년도, 국가를 매칭시켜서 맞는 걸 찾는다.\n",
    "            query_title = title_prev[j].a.find_all('div')[2].find_all('div')[0].text\n",
    "            query_movie = title_prev[j].a.find_all('div')[2].find_all('div')[1].text.split()[0]\n",
    "            query_nation = title_prev[j].a.find_all('div')[2].find_all('div')[1].text.split()[2]\n",
    "            \n",
    "            if(query_title == name_movie_pre and query_movie == year_movie and query_nation == nation_movie): # 세 조건이 맞으면\n",
    "                print(name_movie_pre + '매칭 성공\\n##########################################\\n')\n",
    "                watcha_crawler.get('https://watcha.com' + title_prev[j].a['href']) # 매칭된 영화 페이지에 접속하여\n",
    "                watcha_crawler.save_data() # 데이터를 저장한다.\n",
    "                count+=1\n",
    "                break\n",
    "        \n",
    "        if(count<1):\n",
    "            if(len_same<1):\n",
    "                print('검색 결과가 없음\\n##########################################\\n')\n",
    "            else:\n",
    "                print('검색 결과는 있지만 매칭되는 게 없음\\n##########################################\\n')\n",
    "\n",
    "    \n",
    "    def save_data():\n",
    "        now_html = watcha_crawler.page_source\n",
    "        now_source = BeautifulSoup(now_html, 'lxml')\n",
    "        overview_prev = now_source.find_all('div', {'class':\"css-1jyvmaq-ViewMore et86el20\"})[0]\n",
    "        overview_prev2 = overview_prev.a['href']\n",
    "        time.sleep(0.05)\n",
    "        watcha_crawler.get('https://watcha.com' + overview_prev2)\n",
    "        now_html = watcha_crawler.page_source\n",
    "        now_source = BeautifulSoup(now_html, 'lxml')\n",
    "        overview = now_source.find_all('dd', {'class':\"css-77qx4t-SummaryDetail e1kvv3954\"})[0].text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_html = driver.page_source\n",
    "now_source = BeautifulSoup(now_html, 'lxml')\n",
    "\n",
    "title_prev = now_source.find_all('h1', {'class':\"css-13a04pq-Title e1svyhwg14\" })\n",
    "\n",
    "title = title_prev[0].text\n",
    "\n",
    "temp = driver.current_url\n",
    "\n",
    "overview_prev = now_source.find_all('div', {'class':\"css-1jyvmaq-ViewMore et86el20\"})[0]\n",
    "\n",
    "overview_prev2 = overview_prev.a['href']\n",
    "\n",
    "time.sleep(0.05)\n",
    "\n",
    "driver.get('https://watcha.com' + overview_prev2)\n",
    "\n",
    "now_html = driver.page_source\n",
    "now_source = BeautifulSoup(now_html, 'lxml')\n",
    "\n",
    "overview = now_source.find_all('dd', {'class':\"css-77qx4t-SummaryDetail e1kvv3954\"})[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time.sleep(speed)\n",
    "\n",
    "    try:\n",
    "        self.find_element_by_css_selector(\".u_cbox_in_view_comment\").click() #댓글 보기 누르는 코드\n",
    "        time.sleep(speed)\n",
    "    except exceptions.ElementNotInteractableException as e:\n",
    "        pass\n",
    "    except exceptions.NoSuchElementException as e:\n",
    "        try:\n",
    "            new_addr = dom.find_all('div', {'class' : 'simplecmt_links'})\n",
    "            new_addr = new_addr[0].select('a')[0]['href']\n",
    "            self.get(new_addr)\n",
    "            time.sleep(speed)\n",
    "        except:\n",
    "            pass\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class crawler(webdriver.Chrome):\n",
    "    \n",
    "    def get_input(self):\n",
    "    \n",
    "        press_dict = {'경향신문' : '032' , '국민일보' : '005', '동아일보' : '020', '문화일보' : '021', '서울신문' : '081', \\\n",
    "                      '세계일보' : '022', '조선일보' : '023', '중앙일보' : '025', '한겨레' : '028', '한국일보' : '469'}\n",
    "        print('크롤링을 원하는 언론사를 입력.\\n')\n",
    "        print(\"ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, \\\n",
    "이외의 언론사는 0 입력\\n\")\n",
    "        input_press = input()\n",
    "\n",
    "        if input_press in press_dict.keys():\n",
    "            self.press = input_press\n",
    "            get_number = press_dict[input_press]\n",
    "            print('\\n원하는 날짜를 입력(yyyymmdd)')\n",
    "            input_date = input()\n",
    "            puzzle_url = 'https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=' + get_number + '&date=' + input_date\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(input_press, input_date))\n",
    "\n",
    "        else:\n",
    "            print('\\n입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 \\\n",
    "입력해주세요.\\n')\n",
    "            puzzle_url = input()\n",
    "            self.get(puzzle_url)\n",
    "            input_now = self.page_source\n",
    "            input_source = BeautifulSoup(input_now, 'lxml')\n",
    "            page_list = input_source.find_all('div', {'class' : 'newsflash_header3'})\n",
    "            press_now = page_list[0].h3.text\n",
    "            self.press = press_now\n",
    "            print('\\n{} 맞나요? 원하는 날짜를 입력(yyyymmdd).\\n'.format(press_now))\n",
    "            test_date = input()\n",
    "            print('\\n{}의 {}날짜 뉴스를 크롤링합니다.\\n'.format(press_now, test_date))\n",
    "            puzzle_url = puzzle_url+ '&date=' + test_date\n",
    "\n",
    "        return puzzle_url\n",
    "    \n",
    "    def move_page(self, page_num): # 어떤 날의 여러 페이지 중에 하나로 이동하고 url 을 얻는 method\n",
    "        page_url = puzzle_url + '&page=' + str(page_num)\n",
    "        self.get(page_url)\n",
    "        return page_url\n",
    "    \n",
    "    def list_up(self, html):\n",
    "        \n",
    "        listup = BeautifulSoup(html, 'lxml')\n",
    "        lists = listup.find_all('a', {'class' : 'nclicks(cnt_papaerart)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart3)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_papaerart4)'})\n",
    "        lists += listup.find_all('a', {'class' : 'nclicks(cnt_flashart)'})\n",
    "        \n",
    "        news_list = [article for article in lists if type(article.find('img')) != bs4.element.Tag] # 이미지는 제외\n",
    "        \n",
    "        return news_list\n",
    "    \n",
    "    def break_check(self, news_list, list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "        \n",
    "        if(list_tmp == news_list[0]): \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def exclude_sports_ent(self):\n",
    "        check = self.current_url\n",
    "        if ('sports' in check) or ('entertain' in check):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def get_data(self, speed = 0.2, num_comments = 700): # 제목, 분류, 날짜, 언론사, 내용, 댓글 수집 \n",
    "        # speed는 댓글 더보기를 누르는 간격, 0.1초로 하면 건너뛰는 경우가 있음. \n",
    "        # num_comments는 크롤링하고 싶은 댓글의 수, 삭제된 댓글 포함.\n",
    "        \n",
    "        html = self.page_source\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        current_url = self.current_url\n",
    "\n",
    "        category_raw = dom.find('em', {'class' : 'guide_categorization_item'}) # 분류\n",
    "        category = category_raw.text\n",
    "\n",
    "        title_raw = dom.find_all('h3', {'id' : 'articleTitle'}) # 기사 제목\n",
    "        title = [title.text for title in title_raw]\n",
    "        title = str(title[0])\n",
    "        original_title = title # 제목 원본\n",
    "\n",
    "        title = re.sub('[^0-9a-zA-Zㄱ-힗]', '', title) # 저장시 문제 안생기게 전처리한 제목\n",
    "\n",
    "        date_raw = dom.find_all('span', {'class' : 't11'}) # 날짜\n",
    "        date = date_raw[0].text.split()[0]\n",
    "\n",
    "        press_raw = dom.find('div', {'class' : 'press_logo'}) #언론사\n",
    "        press = self.press\n",
    "\n",
    "        contents_raw = dom.find('div', {'id' : 'articleBodyContents'}) # 뉴스 내용\n",
    "        contents = contents_raw.text\n",
    "\n",
    "        # 네이버 뉴스에는 아래와 같은 주석이 항상 있음. 이 주석을 제거하기 위한 코드\n",
    "        # \\n\\n\\n\\n\\n// flash 오류를 우회하기 위한 함수 추가\\nfunction _flash_removeCallback() {}\\n\\n \n",
    "        clean_index = contents.index('removeCallback') + 23\n",
    "        contents = contents[clean_index :]\n",
    "\n",
    "        # 기사 포맷이 거의 항상 아래와 같음. 필요 없는 정보를 제거하기 위한 코드\n",
    "        # [ⓒ한겨레신문 : 무단전재 및 재배포 금지]\n",
    "        if '재배포' in contents:\n",
    "            reporter_index = contents.index('재배포') - 15\n",
    "            contents = contents[:reporter_index]\n",
    "\n",
    "        time.sleep(speed)\n",
    "        \n",
    "        try:\n",
    "            self.find_element_by_css_selector(\".u_cbox_in_view_comment\").click() #댓글 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.ElementNotInteractableException as e:\n",
    "            pass\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            try:\n",
    "                new_addr = dom.find_all('div', {'class' : 'simplecmt_links'})\n",
    "                new_addr = new_addr[0].select('a')[0]['href']\n",
    "                self.get(new_addr)\n",
    "                time.sleep(speed)\n",
    "            except:\n",
    "                pass\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.find_element_by_css_selector(\".u_cbox_sort_label\").click() #공감순으로 보기 누르는 코드\n",
    "            time.sleep(speed)\n",
    "        except exceptions.NoSuchElementException as e:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            for i in range(num_comments//20):\n",
    "                self.find_element_by_css_selector(\".u_cbox_btn_more\").click() # 댓글 더보기 누르는 코드\n",
    "                time.sleep(speed)\n",
    "        except exceptions.ElementNotVisibleException as e: #댓글 페이지 끝\n",
    "            pass\n",
    "\n",
    "        except Exception as e: # 다른 예외 발생시 확인\n",
    "            pass\n",
    "\n",
    "        html = self.page_source # 댓글 크롤링 코드\n",
    "        dom = BeautifulSoup(html, 'lxml')\n",
    "        comments_raw = dom.find_all('span', {'class' : 'u_cbox_contents'})\n",
    "        comments = [comment.text for comment in comments_raw]\n",
    "\n",
    "        like_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_recomm'}) # 공감수\n",
    "        like_comments = [int(like.text) for like in like_comments_raw]\n",
    "\n",
    "        hate_comments_raw = dom.find_all('em', {'class' : 'u_cbox_cnt_unrecomm'}) # 비공감수\n",
    "        hate_comments = [int(hate.text) for hate in hate_comments_raw]\n",
    "        \n",
    "        if (len(comments)<1): #댓글이 없는 경우\n",
    "            comments = []\n",
    "            like_comments = []\n",
    "            hate_comments = []\n",
    "        \n",
    "        data_list = [category, title, original_title, date, press, contents, comments, like_comments, hate_comments, current_url]\n",
    "        \n",
    "        return data_list\n",
    "    \n",
    "    def save_file(self, data_list):\n",
    "        \n",
    "        file_name = './'+ data_list[4] + '/' + data_list[0]+ '_'  + data_list[4] + '_' + data_list[3] +'_'+ data_list[1] + '.json'\n",
    "        file_data = OrderedDict()\n",
    "        \n",
    "        file_data['url'] = data_list[9]\n",
    "        file_data['press'] = data_list[4]\n",
    "        file_data['date'] = data_list[3]\n",
    "        file_data['category'] = data_list[0]\n",
    "        file_data['title'] = data_list[2]\n",
    "        file_data['contents'] = data_list[5]\n",
    "        file_data['comment'] = data_list[6]\n",
    "        file_data['like'] = data_list[7]\n",
    "        file_data['dont_like'] = data_list[8]\n",
    "\n",
    "        directory = './' + data_list[4]\n",
    "\n",
    "        if os.path.exists(directory):\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data, make_file, ensure_ascii=False, indent='\\t')\n",
    "\n",
    "        else:\n",
    "            os.mkdir(directory)\n",
    "            with open(file_name, 'w', encoding = 'utf-8') as make_file:\n",
    "                json.dump(file_data,  make_file,ensure_ascii=False, indent='\\t')\n",
    "                \n",
    "    def crawl_pages(self, num_page, speed, num_comment): # 크롤링할 페이지 수를 선택, 하루에 약 10페이지 정도 기사가 올라옴\n",
    "                                                         #한 페이지에는 보통 20개의 기사가 있음.\n",
    "        count = 0 # 크롤링한 기사 수 체크용\n",
    "        list_tmp = [0] # 페이지 체크용\n",
    "        \n",
    "        for i in range(num_page):\n",
    "            page_url = self.move_page(i+1)\n",
    "            today_html = self.page_source\n",
    "            news_list = self.list_up(today_html)\n",
    "    \n",
    "            if self.break_check(news_list,list_tmp): #예시) 14페이지와 15페이지의 뉴스리스트가 같다면 break \n",
    "                break\n",
    "            else:\n",
    "                list_tmp = news_list[0]\n",
    "\n",
    "            for index in range(len(news_list)):\n",
    "                try:\n",
    "                    count += 1\n",
    "                    addr = news_list[index]['href']\n",
    "                    self.get(addr)\n",
    "                    # 스포츠 뉴스와 연예 뉴스는 제외 (형식도 다르고 목적과 맞지 않음.)\n",
    "                    if self.exclude_sports_ent():\n",
    "                        continue\n",
    "\n",
    "                    data_list = self.get_data(speed, num_comment)\n",
    "                    print(data_list[9])\n",
    "                    print(\"\\\"{}\\\" 본문과 댓글 {}개를 크롤링.\\n\".format(data_list[2], len(data_list[7])))\n",
    "                    self.save_file(data_list) # 데이터 저장\n",
    "\n",
    "                except:\n",
    "                    print(data_list[9])\n",
    "                    print(\"Error\\n\")\n",
    "                    pass\n",
    "            \n",
    "        return count\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"./chromedriver\"\n",
    "driver = crawler(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링을 원하는 언론사를 입력.\n",
      "\n",
      "ex) 경향신문, 국민일보, 동아일보, 문화일보, 서울신문, 세계일보, 조선일보, 중앙일보, 한겨레, 한국일보, 이외의 언론사는 0 입력\n",
      "\n",
      "0\n",
      "\n",
      "입력한 언론사가 리스트에 없습니다. https://news.naver.com/main/officeList.nhn 에 들어가서 원하는 언론사의 url을 입력해주세요.\n",
      "\n",
      "https://news.naver.com/main/list.nhn?mode=LPOD&mid=sec&oid=003\n",
      "\n",
      "뉴시스 맞나요? 원하는 날짜를 입력(yyyymmdd).\n",
      "\n",
      "20190814\n",
      "\n",
      "뉴시스의 20190814날짜 뉴스를 크롤링합니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "puzzle_url = driver.get_input() # 크롤링하고 싶은 언론사와 날짜를 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401382\n",
      "\"이란 대통령 \"페르시아만 안보에 외국세력 필요 없어\" \" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401381\n",
      "\"뉴욕 증시, 1% 중반 하락세로 개장…독일부진과 '채권역전'  \" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401380\n",
      "\"홍콩 시위로 이틀간 항공편 979편 취소\" 본문과 댓글 24개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401379\n",
      "\"엄용수 의원 2심 선고 후 한 자리에 있었던 '홍준표·조해진'\" 본문과 댓글 7개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401378\n",
      "\"미 국채 수익률에서 장단기 역전 발생…'침체' 전령사? \" 본문과 댓글 2개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401377\n",
      "\"중국군, 홍콩 '10분 거리' 선전에 집결…강경진압 감행하나\" 본문과 댓글 45개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401376\n",
      "\"군위군, 경북 농업인 정보화 경진대회서 5개 부문 수상\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401375\n",
      "\"'추모·기억·연대' 광주 곳곳서 위안부 피해자 기림 행사 열려(종합) \" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401374\n",
      "\"연금복권 424회 1등 6조886447번·1조336233\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401373\n",
      "\"내일은 광복절, 우리나라를 생각해봐요\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401372\n",
      "\"해군 1함대, 815 태극기 퍼포먼스\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401371\n",
      "\"나라사랑은 태극기 게양부터 시작해요\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401370\n",
      "\"'위안부 기림일' 폭염 속 1400차 수요집회…日정부 규탄 행사 봇물(종합)\" 본문과 댓글 15개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401369\n",
      "\"김재규 강원경찰청장, 속초 승강기 사고 현장 찾아\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401368\n",
      "\"속초소방 구조대, 승강기 추락사고 현장서 긴급 구조\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401367\n",
      "\"속초소방 구급대 환자 이송 긴급 대기 \" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401366\n",
      "\"시각장애인들 스쿠버 장비 착용하고 바다에 풍덩\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401365\n",
      "\"시각장애인의 스쿠버다이빙 체험\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401364\n",
      "\"\"행정안전부 감사합니다\"\" 본문과 댓글 0개를 크롤링.\n",
      "\n",
      "https://news.naver.com/main/read.nhn?mode=LPOD&mid=sec&oid=003&aid=0009401363\n",
      "\"시각장애인들의 내 인생 첫 바닷속 체험\" 본문과 댓글 1개를 크롤링.\n",
      "\n",
      "################ number of articles: 20 ################\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    count = driver.crawl_pages(1,0.2,50) #parameter 순서\n",
    "                                         #크롤링할 페이지 수(한 페이지에는 약 20개 기사가 있음.)\n",
    "                                         #크롤링 속도 조절(0.1로 하면 에러나기도 함)\n",
    "                                         #크롤링할 댓글 수 (삭제된 댓글 포함, 약간의 오차 있을 수 있음.)\n",
    "    print('################ number of articles: {} ################'.format(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
